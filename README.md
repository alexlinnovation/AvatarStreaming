# Ditto: Custom Talking Head Synthesis

<div align="center">
    <video style="width: 95%; object-fit: cover;" controls loop src="https://github.com/user-attachments/assets/ef1a0b08-bff3-4997-a6dd-62a7f51cdb40" muted="false"></video>
</div>

A full-stack application for real-time talking head synthesis with a Next.js frontend and Python backend powered by TensorRT inference.

## Project Structure
```text
ROOT DIRECTORY/
â”œâ”€â”€ frontend/                    (Next.js frontend application)
â”‚   â”œâ”€â”€ src/                    (Source code)
â”‚   â”œâ”€â”€ pages/                  (Next.js pages)
â”‚   â”œâ”€â”€ styles/                 (Styling files)
â”‚   â”œâ”€â”€ transcriptions/         (Transcription files)
â”‚   â”œâ”€â”€ env/                    (Environment configurations)
â”‚   â””â”€â”€ package.json            (Frontend dependencies)
â”‚
â”œâ”€â”€ src/                        (Python backend)
â”‚   â”œâ”€â”€ __pvcache__/            (Cache directory)
â”‚   â”œâ”€â”€ static/                 (Static files)
â”‚   â”œâ”€â”€ tests/                  (Test files)
â”‚   â”œâ”€â”€ uploads/                (Upload directory)
â”‚   â”œâ”€â”€ app.py                  (Main application)
â”‚   â”œâ”€â”€ inference.py            (Inference logic)
â”‚   â”œâ”€â”€ livekit_video.py        (LiveKit integration)
â”‚   â””â”€â”€ livekit_video_keepalive.py (Keepalive service)
â”‚
â”œâ”€â”€ scripts/                    (Utility scripts)
â”‚   â”œâ”€â”€ gated_avsync.py         (Audio-video sync)
â”‚   â”œâ”€â”€ gated_avsyncv2.py       (Enhanced AV sync)
â”‚   â”œâ”€â”€ qwen.py                 (Qwen model integration)
â”‚   â””â”€â”€ utils.py                (Utility functions)
â”‚
â”œâ”€â”€ .env.example                (Environment template)
â”œâ”€â”€ .eslintrc.json              (ESLint configuration)
â”œâ”€â”€ .gitignore                  (Git ignore rules)
â”œâ”€â”€ Dockerfile                  (Docker configuration)
â”œâ”€â”€ package.json                (Project dependencies)
â”œâ”€â”€ README.md                   (This file)
â”œâ”€â”€ requirements.txt            (Python dependencies)
â””â”€â”€ tsconfig.json               (TypeScript config)
```

## ğŸ› ï¸ Installation

Tested Environment  
- System: Ubuntu
- GPU: RTX 3090  
- Python: 3.10  
- tensorRT: 8.6.1  

## ğŸ“¥ Docker
Check the
```bash
Dockerfile
```

## ğŸ› ï¸ Installation

### System Requirements
- Base Image: nvcr.io/nvidia/tensorrt:23.12-py3
- System: Ubuntu
- GPU: RTX 3090 (or compatible)
- Python: 3.10
- TensorRT: 8.6.1

### System Dependencies
- build-essential
- python3-dev python3.10-dev
- ffmpeg
- libsndfile1
- libturbojpeg0-dev
- libsm6 libxext6 libgl1
- git curl wget

### Python Dependencies
- numpy==2.0.1
- kokoro-onnx==0.3.9
- onnxruntime-gpu==1.22.0
- Other requirements from requirements.txt

### Installation Methods

#### Docker Setup
Check the Dockerfile for containerized deployment.

#### Conda Environment
Create conda environment:
conda env create -f environment.yaml
conda activate ditto

#### Pip Installation
pip install \
    tensorrt==8.6.1 \
    librosa \
    tqdm \
    filetype \
    imageio \
    opencv_python_headless \
    scikit-image \
    cython \
    cuda-python \
    imageio-ffmpeg \
    colored \
    polygraphy \
    numpy==2.0.1


## ğŸš€ Running the Application

### Backend Server
Start the Python backend:
uvicorn livekit_video_keepalive:app --host 0.0.0.0 --port 8010 --reload

### Frontend Development
Start the Next.js frontend:
cd frontend
npm install
npm run dev


### Conda
Create `conda` environment:
```bash
conda env create -f environment.yaml
conda activate ditto
```

### Pip
```bash
pip install \
    tensorrt==8.6.1 \
    librosa \
    tqdm \
    filetype \
    imageio \
    opencv_python_headless \
    scikit-image \
    cython \
    cuda-python \
    imageio-ffmpeg \
    colored \
    polygraphy \
    numpy==2.0.1
```

## ğŸ“¥ Download Checkpoints
Download checkpoints from [HuggingFace](https://huggingface.co/digital-avatar/ditto-talkinghead) and put them in `checkpoints` dir:
```bash
git lfs install
git clone https://huggingface.co/digital-avatar/ditto-talkinghead checkpoints
```

The `checkpoints` should be like:
```text
./checkpoints/
â”œâ”€â”€ ditto_cfg
â”‚Â Â  â”œâ”€â”€ v0.4_hubert_cfg_trt.pkl
â”‚Â Â  â””â”€â”€ v0.4_hubert_cfg_trt_online.pkl
â”œâ”€â”€ ditto_onnx
â”‚Â Â  â”œâ”€â”€ appearance_extractor.onnx
â”‚Â Â  â”œâ”€â”€ blaze_face.onnx
â”‚Â Â  â”œâ”€â”€ decoder.onnx
â”‚Â Â  â”œâ”€â”€ face_mesh.onnx
â”‚Â Â  â”œâ”€â”€ hubert.onnx
â”‚Â Â  â”œâ”€â”€ insightface_det.onnx
â”‚Â Â  â”œâ”€â”€ landmark106.onnx
â”‚Â Â  â”œâ”€â”€ landmark203.onnx
â”‚Â Â  â”œâ”€â”€ libgrid_sample_3d_plugin.so
â”‚Â Â  â”œâ”€â”€ lmdm_v0.4_hubert.onnx
â”‚Â Â  â”œâ”€â”€ motion_extractor.onnx
â”‚Â Â  â”œâ”€â”€ stitch_network.onnx
â”‚Â Â  â””â”€â”€ warp_network.onnx
â””â”€â”€ ditto_trt_Ampere_Plus
    â”œâ”€â”€ appearance_extractor_fp16.engine
    â”œâ”€â”€ blaze_face_fp16.engine
    â”œâ”€â”€ decoder_fp16.engine
    â”œâ”€â”€ face_mesh_fp16.engine
    â”œâ”€â”€ hubert_fp32.engine
    â”œâ”€â”€ insightface_det_fp16.engine
    â”œâ”€â”€ landmark106_fp16.engine
    â”œâ”€â”€ landmark203_fp16.engine
    â”œâ”€â”€ lmdm_v0.4_hubert_fp32.engine
    â”œâ”€â”€ motion_extractor_fp32.engine
    â”œâ”€â”€ stitch_network_fp16.engine
    â””â”€â”€ warp_network_fp16.engine
```

- The `ditto_cfg/v0.4_hubert_cfg_trt_online.pkl` is online config
- The `ditto_cfg/v0.4_hubert_cfg_trt.pkl` is offline config


## ğŸ“š Citation
If you find this codebase useful for your research, please use the following entry.
```BibTeX
@article{li2024ditto,
    title={Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis},
    author={Li, Tianqi and Zheng, Ruobing and Yang, Minghui and Chen, Jingdong and Yang, Ming},
    journal={arXiv preprint arXiv:2411.19509},
    year={2024}
}
```
